{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"cnn_CIFAR","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOo/8bjsZ4ohlfLT3Q59EXC"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Tr45ZSuZjd7","outputId":"1b70949c-1770-491d-8553-98b918eeb9e3"},"source":["import numpy as np\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv = nn.Sequential (\n","            # 32-3+1=30*30*16\n","            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n","            nn.ReLU(inplace=True),\n","            # 30-3+1=28*28*16\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n","            nn.ReLU(inplace=True),\n","            # 28-3+1=26*26*64\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n","            nn.BatchNorm2d(64),\n","            # 13*13*64\n","            nn.MaxPool2d(2,2),\n","            nn.ReLU(inplace=True),\n","            # 13-2+1=12*12*128\n","            nn.Conv2d(in_channels=64,out_channels=128, kernel_size=2),\n","            nn.BatchNorm2d(128),\n","            # 6*6*128\n","            nn.MaxPool2d(2,2),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.05)\n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(4608, 1024),\n","            nn.Dropout(0.05),\n","            nn.Linear(1024,128)\n","        )\n","\n","\n","    def forward(self, x):\n","        x = x.cuda()\n","        x = self.conv(x)\n","        x = x.view(x.shape[0],-1)\n","        x = self.fc(x)\n","        return x\n","\n","PATH = './model.pth'\n","\n","def main():\n","    batch_size, num_epoch = 8, 15\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    print(device)\n","    # load and transform dataset\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                            download=True, transform=transform)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                            shuffle=True, num_workers=2)\n","\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                        download=True, transform=transform)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                            shuffle=False, num_workers=2)\n","\n","    classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    net = CNN()\n","    net.to(device)\n","   \n","    criterion = nn.CrossEntropyLoss().to(device)\n","    optimizer = optim.Adam(net.parameters(), lr=0.001)\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","    for epoch in range(num_epoch):  # loop over the dataset multiple times\n","        for param_group in optimizer.param_groups:\n","            print(\"Epoch {}, learning rate: {}\".format(epoch, param_group['lr']))\n","        scheduler.step()\n","\n","        running_loss = 0.0\n","        \n","        for i, data in enumerate(trainloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 1000 == 999:    # print every 2000 mini-batches\n","                print('[%d, %5d] loss: %.3f' %\n","                    (epoch + 1, i + 1, running_loss / 2000))\n","                running_loss = 0.0\n","\n","    print('Finished Training')\n","\n","    torch.save(net.state_dict(), PATH)\n","\n","\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the 10000 test images: %d %%' % (\n","        100 * correct / total))\n","\n","           \n","if __name__ == \"__main__\":\n","    main()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 0, learning rate: 0.001\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[1,  1000] loss: 1.182\n","[1,  2000] loss: 0.847\n","[1,  3000] loss: 0.796\n","[1,  4000] loss: 0.738\n","[1,  5000] loss: 0.688\n","[1,  6000] loss: 0.641\n","Epoch 1, learning rate: 0.001\n","[2,  1000] loss: 0.604\n","[2,  2000] loss: 0.602\n","[2,  3000] loss: 0.590\n","[2,  4000] loss: 0.582\n","[2,  5000] loss: 0.572\n","[2,  6000] loss: 0.561\n","Epoch 2, learning rate: 0.001\n","[3,  1000] loss: 0.525\n","[3,  2000] loss: 0.533\n","[3,  3000] loss: 0.537\n","[3,  4000] loss: 0.528\n","[3,  5000] loss: 0.518\n","[3,  6000] loss: 0.513\n","Epoch 3, learning rate: 0.001\n","[4,  1000] loss: 0.504\n","[4,  2000] loss: 0.497\n","[4,  3000] loss: 0.490\n","[4,  4000] loss: 0.489\n","[4,  5000] loss: 0.495\n","[4,  6000] loss: 0.487\n","Epoch 4, learning rate: 0.001\n","[5,  1000] loss: 0.439\n","[5,  2000] loss: 0.451\n","[5,  3000] loss: 0.457\n","[5,  4000] loss: 0.447\n","[5,  5000] loss: 0.464\n","[5,  6000] loss: 0.459\n","Epoch 5, learning rate: 0.0009000000000000001\n","[6,  1000] loss: 0.427\n","[6,  2000] loss: 0.434\n","[6,  3000] loss: 0.422\n","[6,  4000] loss: 0.439\n","[6,  5000] loss: 0.445\n","[6,  6000] loss: 0.446\n","Epoch 6, learning rate: 0.0009000000000000001\n","[7,  1000] loss: 0.412\n","[7,  2000] loss: 0.417\n","[7,  3000] loss: 0.419\n","[7,  4000] loss: 0.421\n","[7,  5000] loss: 0.429\n","[7,  6000] loss: 0.420\n","Epoch 7, learning rate: 0.0009000000000000001\n","[8,  1000] loss: 0.398\n","[8,  2000] loss: 0.399\n","[8,  3000] loss: 0.409\n","[8,  4000] loss: 0.405\n","[8,  5000] loss: 0.399\n","[8,  6000] loss: 0.415\n","Epoch 8, learning rate: 0.0009000000000000001\n","[9,  1000] loss: 0.387\n","[9,  2000] loss: 0.397\n","[9,  3000] loss: 0.393\n","[9,  4000] loss: 0.390\n","[9,  5000] loss: 0.407\n","[9,  6000] loss: 0.400\n","Epoch 9, learning rate: 0.0009000000000000001\n","[10,  1000] loss: 0.366\n","[10,  2000] loss: 0.377\n","[10,  3000] loss: 0.369\n","[10,  4000] loss: 0.375\n","[10,  5000] loss: 0.379\n","[10,  6000] loss: 0.387\n","Epoch 10, learning rate: 0.0008100000000000001\n","[11,  1000] loss: 0.352\n","[11,  2000] loss: 0.361\n","[11,  3000] loss: 0.365\n","[11,  4000] loss: 0.371\n"],"name":"stdout"}]}]}