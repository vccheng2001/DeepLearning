{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"cnn_CIFAR","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNvOfHv34hXS8oe8jXS7p7d"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Tr45ZSuZjd7","executionInfo":{"status":"ok","timestamp":1614550458780,"user_tz":480,"elapsed":615599,"user":{"displayName":"Vivian Cheng","photoUrl":"","userId":"10374517085012709020"}},"outputId":"e079133b-f2ce-4d27-d3c3-0d9315d12ed6"},"source":["import numpy as np\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv = nn.Sequential (\n","            # 32-3+1=30*30*16\n","            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n","            nn.ReLU(inplace=True),\n","            # 30-3+1=28*28*16\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n","            nn.ReLU(inplace=True),\n","            # 28-3+1=26*26*64\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n","            nn.BatchNorm2d(64),\n","            # 13*13*64\n","            nn.MaxPool2d(2,2),\n","            nn.ReLU(inplace=True),\n","            # 13-2+1=12*12*128\n","            nn.Conv2d(in_channels=64,out_channels=128, kernel_size=2),\n","            nn.BatchNorm2d(128),\n","            # 6*6*128\n","            nn.MaxPool2d(2,2),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.05)\n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(4608, 1024),\n","            nn.Dropout(0.05),\n","            nn.Linear(1024,128)\n","        )\n","\n","\n","    def forward(self, x):\n","        x = x.cuda()\n","        x = self.conv(x)\n","        x = x.view(x.shape[0],-1)\n","        x = self.fc(x)\n","        return x\n","\n","PATH = './model.pth'\n","\n","def main():\n","    batch_size, num_epoch = 8, 15\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    print(device)\n","    # load and transform dataset\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                            download=True, transform=transform)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                            shuffle=True, num_workers=2)\n","\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                        download=True, transform=transform)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                            shuffle=False, num_workers=2)\n","\n","    classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    net = CNN()\n","    net.to(device)\n","   \n","    criterion = nn.CrossEntropyLoss().to(device)\n","    # optimizer = optim.Adam(net.parameters(), lr=0.002)\n","    # # torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","    # scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","    optimizer = optim.SGD(net.parameters(), lr=0.0025, momentum=0.9)\n","    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","\n","    for epoch in range(num_epoch):  # loop over the dataset multiple times\n","        for param_group in optimizer.param_groups:\n","            print(\"Epoch {}, learning rate: {}\".format(epoch, param_group['lr']))\n","        scheduler.step(epoch)\n","\n","        running_loss = 0.0\n","        \n","        for i, data in enumerate(trainloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 1000 == 999:    # print every 2000 mini-batches\n","                print('[%d, %5d] loss: %.3f' %\n","                    (epoch + 1, i + 1, running_loss / 2000))\n","                running_loss = 0.0\n","\n","    print('Finished Training')\n","\n","    torch.save(net.state_dict(), PATH)\n","\n","\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the 10000 test images: %d %%' % (\n","        100 * correct / total))\n","\n","           \n","if __name__ == \"__main__\":\n","    main()\n","\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["cuda\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 0, learning rate: 0.0025\n","[1,  1000] loss: 1.023\n","[1,  2000] loss: 0.837\n","[1,  3000] loss: 0.742\n","[1,  4000] loss: 0.671\n","[1,  5000] loss: 0.628\n","[1,  6000] loss: 0.604\n","Epoch 1, learning rate: 0.0025\n","[2,  1000] loss: 0.560\n","[2,  2000] loss: 0.525\n","[2,  3000] loss: 0.523\n","[2,  4000] loss: 0.509\n","[2,  5000] loss: 0.486\n","[2,  6000] loss: 0.497\n","Epoch 2, learning rate: 0.0025\n","[3,  1000] loss: 0.447\n","[3,  2000] loss: 0.448\n","[3,  3000] loss: 0.442\n","[3,  4000] loss: 0.443\n","[3,  5000] loss: 0.439\n","[3,  6000] loss: 0.433\n","Epoch 3, learning rate: 0.0025\n","[4,  1000] loss: 0.382\n","[4,  2000] loss: 0.385\n","[4,  3000] loss: 0.385\n","[4,  4000] loss: 0.408\n","[4,  5000] loss: 0.395\n","[4,  6000] loss: 0.407\n","Epoch 4, learning rate: 0.0025\n","[5,  1000] loss: 0.348\n","[5,  2000] loss: 0.352\n","[5,  3000] loss: 0.350\n","[5,  4000] loss: 0.368\n","[5,  5000] loss: 0.359\n","[5,  6000] loss: 0.367\n","Epoch 5, learning rate: 0.0025\n","[6,  1000] loss: 0.311\n","[6,  2000] loss: 0.313\n","[6,  3000] loss: 0.323\n","[6,  4000] loss: 0.336\n","[6,  5000] loss: 0.338\n","[6,  6000] loss: 0.333\n","Epoch 6, learning rate: 0.0025\n","[7,  1000] loss: 0.281\n","[7,  2000] loss: 0.295\n","[7,  3000] loss: 0.307\n","[7,  4000] loss: 0.300\n","[7,  5000] loss: 0.315\n","[7,  6000] loss: 0.321\n","Epoch 7, learning rate: 0.0025\n","[8,  1000] loss: 0.253\n","[8,  2000] loss: 0.266\n","[8,  3000] loss: 0.281\n","[8,  4000] loss: 0.294\n","[8,  5000] loss: 0.288\n","[8,  6000] loss: 0.298\n","Epoch 8, learning rate: 0.0025\n","[9,  1000] loss: 0.228\n","[9,  2000] loss: 0.258\n","[9,  3000] loss: 0.265\n","[9,  4000] loss: 0.270\n","[9,  5000] loss: 0.278\n","[9,  6000] loss: 0.266\n","Epoch 9, learning rate: 0.0025\n","[10,  1000] loss: 0.217\n","[10,  2000] loss: 0.241\n","[10,  3000] loss: 0.236\n","[10,  4000] loss: 0.250\n","[10,  5000] loss: 0.255\n","[10,  6000] loss: 0.266\n","Epoch 10, learning rate: 0.0025\n","[11,  1000] loss: 0.202\n","[11,  2000] loss: 0.215\n","[11,  3000] loss: 0.226\n","[11,  4000] loss: 0.233\n","[11,  5000] loss: 0.243\n","[11,  6000] loss: 0.244\n","Epoch 11, learning rate: 0.0025\n","[12,  1000] loss: 0.159\n","[12,  2000] loss: 0.150\n","[12,  3000] loss: 0.147\n","[12,  4000] loss: 0.138\n","[12,  5000] loss: 0.134\n","[12,  6000] loss: 0.130\n","Epoch 12, learning rate: 0.00025\n","[13,  1000] loss: 0.120\n","[13,  2000] loss: 0.128\n","[13,  3000] loss: 0.125\n","[13,  4000] loss: 0.117\n","[13,  5000] loss: 0.125\n","[13,  6000] loss: 0.122\n","Epoch 13, learning rate: 0.00025\n","[14,  1000] loss: 0.116\n","[14,  2000] loss: 0.113\n","[14,  3000] loss: 0.110\n","[14,  4000] loss: 0.113\n","[14,  5000] loss: 0.117\n","[14,  6000] loss: 0.112\n","Epoch 14, learning rate: 0.00025\n","[15,  1000] loss: 0.100\n","[15,  2000] loss: 0.107\n","[15,  3000] loss: 0.106\n","[15,  4000] loss: 0.114\n","[15,  5000] loss: 0.105\n","[15,  6000] loss: 0.110\n","Finished Training\n","Accuracy of the network on the 10000 test images: 73 %\n"],"name":"stdout"}]}]}